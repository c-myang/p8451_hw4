---
title: "Machine Learning for Epi: Assignment 4"
output:
  word_document: default
  html_document: default
date: "2023-02-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = F,
                      message = F)

library(tidyverse)
library(caret)
library(ggbiplot)
library(stats)
library(factoextra)
library(cluster)
```

## Part I: Implementing a Simple Prediction Pipeline

The New York City Department of Health administered a questionnaire on general health and physical activity among residents. 

### Step 1: Load data and prepare for analysis

The code chunk below loads the Physical Activity and Transit (PAT) survey data, renames variables to meaningful names, and when appropriate, converts numeric variables to factors.

```{r load_data}
PAT_df = readr::read_csv("./class4_p1.csv") %>% 
  dplyr::rename(id = `...1`,
                htn = chronic1,
                diabetes = chronic3,
                asthma = chronic4,
                tobacco = tobacco1,
                alcohol = alcohol1,
                pa_min = gpaq8totmin,
                walk_days = gpaq11days,
                physical_act = habits5,
                diet = habits7,
                gender = dem3,
                hisp_lat = dem4,
                us_born = dem8) %>% 
  mutate_at(vars(htn:asthma, tobacco:alcohol, physical_act:povertygroup), ~ as.factor(.)) 

summary(PAT_df) %>% knitr::kable(digits = 2)
```

Based on the summary of the output, we can see that the features BMI, `pa_min` (minutes of physical activity on chores), and `walk_days` are continuous. Meanwhile, the remaining features are factor variables (`htn`, `diabetes`, `asthma`, `tobacco`, `alcohol`, `physical_act`, `diet`, `gender`, `hisp_lat`, `us_born`, `povertygroup`).

### Step 2: Pre-process the data

Next, we will preprocess the data. The code chunk below removes the `id` feature, omits `NA` observations. We will also want to center and scale the data, however, the `caret` package will do this for us within the model training step.

```{r preprocess}
set.seed(123)

#Drop category and ID variable and remove missings
PAT_df = PAT_df %>% 
  select(-id) %>% 
  na.omit()
```


### Step 3: Partition the data

The code chunk below partitions the data into training and testing sets, using a 70/30 split. 

```{r partition_data}
#Creating balanced partitions in the data
train_index = createDataPartition(PAT_df$healthydays, p = 0.7, list = FALSE)

PAT_train = PAT_df[train_index,]
PAT_test = PAT_df[-train_index,]

#Check distribution of the outcome between train and test data
summary(PAT_test$healthydays) 
summary(PAT_train$healthydays)
```

We can see that the training and testing sets have similar median, mean, and IQR ranges, indicating that the data were successfully partitioned.

### Step 4: Construct linear regression models to predict healthy days

We will fit two linear models to predict the number of days in a month an individual reported having good physical health (feature name: `healthydays`).

- Model 1 (`lm_all`): Include all features

- Model 2 (`lm_habits`): Include only health habits and physical activity variables (tobacco + alcohol + pa_min + walk_days + physical_act + diet)

These models will be trained on the training dataset using 10-fold cross validation. The data will be centered and scaled within the  `preProc` option within `train()`. 

```{r lm}
#Perform 10-fold cross-validation
control.settings = trainControl(method = "cv", number = 10)

#Fit models using caret
lm_all = train(healthydays ~., data = PAT_train, method = "lm", preProc = c("center", "scale"), trControl = control.settings)

lm_habits = train(healthydays ~ tobacco + alcohol + pa_min + walk_days + physical_act + diet, data = PAT_train, method = "lm", preProc = c("center", "scale"), trControl = control.settings)

# Output results
rbind(lm_all$results, lm_habits$results) %>% 
  mutate(Model = c("All", "Habits")) %>% 
  relocate(Model, .before = intercept) %>% 
  knitr::kable(digits = 3)
```

Results of the cross-validated training shows the model with all variables, `lm_all`, has a slightly lower RMSE compared to the model with only health habits. However, we want to confirm this by running each model on the testing data.

### Step 5: Model Evaluation

Next, to determine the preferred prediction model, we will apply both models to the test data, and compare their performance based on the RMSE, which is an evaluation metric used for linear regression models.

```{r test}
test_outcome1 = predict(lm_all, PAT_test)
All_RMSE = RMSE(test_outcome1, PAT_test$healthydays)

test_outcome2 = predict(lm_habits, PAT_test)
Habits_RMSE = RMSE(test_outcome2, PAT_test$healthydays)

cbind(All_RMSE, Habits_RMSE) %>% 
  as_tibble() %>% 
  knitr::kable(digits = 3)
```

The table shows that the `lm_all` model performs better on the test data, with an RMSE of 7.172, compared to the `lm_habits` model's RMSE of 7.413. Therefore, if I were only interested in prediction performance, I would select the linear model with all features over the linear model with health habit features to predict the number of healthy days in a month.

One setting where the `lm_all` model would be useful would be in a program evaluation setting. For instance, in a health program that promotes increased physical activity, when enrolling new subjects, we could predict the number of healthy days they are predicted to have in a month at baseline, and then compare the number of observed healthy days after program participation. This provides evidence for evaluating the success of the physical activity program in improving the number of healthy days a person may have.

## Part II: Conducting an Unsupervised Analysis

In Part II, we will use a data-driven hierarchical clustering approach to identify the optimal number of clusters in the R dataset, `USArrests`. The data includes the crime statistics for each of the 50 US states in 1973. Incidence of arrest, per 100,000 residents for assault, murder and rape are included along with the proportion of the population that lives in urban communities.

### Transform data

First, we will load the data and check the means and SD to determine whether scaling and centering our data is necessary:

```{r data_load}
USArrests = USArrests %>% na.omit()
summary(USArrests) %>% knitr::kable()

#Check means and SDs to determine if scaling is necessary
tibble(rate = c("Murder",  "Assault", "UrbanPop", "Rape"),
       means = colMeans(USArrests, na.rm = TRUE), 
       sd = apply(USArrests, 2, sd, na.rm = TRUE)) %>%
  knitr::kable(digits = 2)

```

We can see that the means differ significantly across the features of murder, assault, and rape rates, as well as the urban population. Therefore, we should scale and center our features. The code chunk below centers and scales the features in the USArrests and outputs the transformed data into a `transformed_USArrests` dataframe.

```{r transform}
#Centering and Scaling
USArrests_numeric = USArrests %>% dplyr::select(where(is.numeric))
USArrests_preprocess = preProcess(USArrests_numeric, method = c("center", "scale"))

#Output pre-processed values
transformed_USArrests = predict(USArrests_preprocess, USArrests_numeric)
transformed_USArrests %>% head()
```

### Hierarchical clustering analysis

Now with the transformed USArrests data, we will conduct the hierarchical clustering analysis. The code chunk below creates a dissimilarity matrix based on **Euclidean** distances. We will then apply a **complete** linkage agglomeration method to measure the dissimilarity between clusters.

```{r hclust}
set.seed(123)

# Create Dissimilarity matrix
diss_matrix = dist(transformed_USArrests, method = "euclidean")

# Hierarchical clustering using Complete Linkage
clusters_h = hclust(diss_matrix, method = "complete")

# Plot the obtained dendrogram
plot(clusters_h, cex = 0.6, hang = -1)
```

The above dendrogram shows the result of our hierarchical clustering analysis. We can see that the most granular clusters each contain 1-2 observations, and as we move up the tree, similar observations are combined into branches.

Finally, to determine the optimal number of clusters, we will use the gap statistic method.

```{r gapstat}
#Calculate gap statistic
gap_stat = clusGap(transformed_USArrests, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

Based on the gap-statistic plot, we can see the optimal number of clusters is 3. Let's use number of clusters from gap statistic to visualize each cluster in our dendrogram.

```{r visualize_clusters}
clusters_h_3 = cutree(clusters_h, k = 3)
table(clusters_h_3)

plot(clusters_h, cex = 0.6)
rect.hclust(clusters_h, k = 3, border = 2:5) 
```

The resulting table and plot shows marked differences in the number observations per cluster, with 8 in the 1st cluster, and 31 in the third cluster. To inspect further, we can examine the feature characteristics from sample of 5 observations from each cluster in our data.

```{r clust_table}
USArrests %>%
  mutate(cluster = clusters_h_3) %>%
  group_by(cluster) %>% 
  slice_sample(n = 5) %>% 
  knitr::kable()
```

We can see that cluster 1 is characterized by cities with a relatively low urban population (45-60%) and higher rates of murder (10-17.4 per 100,000). Cluster 2 is characterized by cities with a high urban population (67-91%), higher rates of rape and assault. Finally, cluster 3 is characterized by a medium urban population (32-80%), and relatively lower rates of murder (2.2-8.5 per 100,000), assault, and rape.

### Research Application of Clustering Analysis

Assuming these data are from 2020, we could potentially use these data to identify cities that vary in key crime characteristics, and design cluster-specific crime reduction programs that best suit the crime-related features of each cluster. Our research question would be whether designing crime prevention programs according to cities' urban population, and rates of assault, rape, and murder are effective in reducing such crime rates. In this instance, clusters would serve as an exposure in identifying the appropriate crime prevention program to be applied to each city.

Before conducting this research, there are clearly some ethical considerations we should consider. For instance, we should consider the inherent biases present in the data, such as the factors that may lead to over- or under-reporting of crime statistics across cities. The implications of relying on invalid, biased data in a cluster analysis may justify excessive punitive crime prevention efforts in certain cities, reinforce existing crime-related stigmas or biases of different cities, and risk deeming high-crime clusters as 'problematic' for law enforcement and policymakers. 

